{
  "metadata": {},
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2da35050",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "# Water Quality Prediction — Submission Notebook"
    },
    {
      "cell_type": "code",
      "id": "6d008edf",
      "metadata": {
        "language": "python"
      },
      "source": "!pip install uv\n!uv pip install -r requirements.txt xgboost shap ",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "1dd1c936",
      "metadata": {
        "language": "python"
      },
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\n# Multi-dimensional arrays and datasets (e.g., NetCDF, Zarr)\nimport xarray as xr\n\n# Geospatial raster data handling with CRS support\nimport rioxarray as rxr\n\n# Raster operations and spatial windowing\nimport rasterio\nfrom rasterio.windows import Window\n\n# Feature preprocessing and data splitting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GroupKFold, RandomizedSearchCV\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom scipy.spatial import cKDTree\nfrom scipy.stats import uniform, randint, mstats\nimport shap\n\n# Machine Learning\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, make_scorer\n\n\n# Planetary Computer tools for STAC API access and authentication\nimport pystac_client\nimport planetary_computer as pc\nfrom odc.stac import stac_load\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom datetime import date\nfrom tqdm import tqdm\nimport os",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "30c2dfb0",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 1. Load & Merge Data (join by key, not by index)"
    },
    {
      "cell_type": "code",
      "id": "30f4ae0e",
      "metadata": {
        "language": "python"
      },
      "source": "wq = pd.read_csv(\"water_quality_training_dataset.csv\")\nlandsat = pd.read_csv(\"landsat_features_training.csv\")\nterra = pd.read_csv(\"terraclimate_features_training.csv\")\n\n# Merge by key — never by index\nJOIN_KEYS = ['Latitude', 'Longitude', 'Sample Date']\ndf = wq.merge(landsat, on=JOIN_KEYS, how='left') \\\n       .merge(terra, on=JOIN_KEYS, how='left')\n\nprint(f\"Shape after merge: {df.shape}\")\ndisplay(df.head())",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "45c1523d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 2. Landsat Calibration (DN → Reflectance) & Recalculate Indices"
    },
    {
      "cell_type": "code",
      "id": "22b64ae5",
      "metadata": {
        "language": "python"
      },
      "source": "BANDS = ['nir', 'green', 'swir16', 'swir22']\n\ndef calibrate_landsat(data):\n    \"\"\"Apply Landsat C2L2 scale/offset to convert DN → surface reflectance.\"\"\"\n    scale, offset = 0.0000275, -0.2\n    for col in BANDS:\n        data[col] = data[col] * scale + offset\n    # Recalculate indices from calibrated reflectance\n    data['NDMI']  = (data['nir'] - data['swir16']) / (data['nir'] + data['swir16'])\n    data['MNDWI'] = (data['green'] - data['swir22']) / (data['green'] + data['swir22'])\n    return data\n\ndf = calibrate_landsat(df)\nprint(\"Calibrated reflectance ranges:\")\nfor b in BANDS:\n    print(f\"  {b}: [{df[b].min():.4f}, {df[b].max():.4f}]\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "e15b5e6d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 3. Cloud Filter, Cluster-based Imputation & DRP Censoring"
    },
    {
      "cell_type": "code",
      "id": "2ee8ff34",
      "metadata": {
        "language": "python"
      },
      "source": "# Cloud filter: after calibration, valid reflectance is roughly [0, 1.0]\nfor col in BANDS:\n    df[col] = df[col].where(df[col].between(-0.05, 1.2), np.nan)\n\n# Cluster-based imputation (spatial, not global median)\nN_CLUSTERS = 8\nkmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\ndf['geo_cluster'] = kmeans.fit_predict(df[['Latitude', 'Longitude']])\n\nimpute_cols = BANDS + ['NDMI', 'MNDWI', 'pet']\nfor col in impute_cols:\n    cluster_med = df.groupby('geo_cluster')[col].transform('median')\n    df[col] = df[col].fillna(cluster_med)\n# Fallback: global median for any remaining NaN\ntrain_medians = df[impute_cols].median()\ndf[impute_cols] = df[impute_cols].fillna(train_medians)\n\n# DRP censoring: 10.0 = below detection limit → replace with DL/2\ndf['Dissolved Reactive Phosphorus'] = df['Dissolved Reactive Phosphorus'].replace(10.0, 5.0)\n\nprint(f\"Remaining NaNs:\\n{df.isna().sum()[df.isna().sum() > 0]}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d64725f5",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 4. Feature Engineering"
    },
    {
      "cell_type": "code",
      "id": "059d9978",
      "metadata": {
        "language": "python"
      },
      "source": "dates = pd.to_datetime(df['Sample Date'], dayfirst=True)\ndf['month_sin'] = np.sin(2 * np.pi * dates.dt.month / 12)\ndf['month_cos'] = np.cos(2 * np.pi * dates.dt.month / 12)\ndf['year']      = dates.dt.year\ndf['NIR_SWIR22_ratio'] = df['nir'] / (df['swir22'] + 1e-6)\ndf['turbidity']        = df['swir16'] / (df['green'] + 1e-6)\n\n# PET seasonal lags (proxy for antecedent moisture)\ndf = df.sort_values(['Latitude', 'Longitude', 'Sample Date']).reset_index(drop=True)\ngrp = df.groupby(['Latitude', 'Longitude'])['pet']\ndf['pet_lag1']     = grp.shift(1)\ndf['pet_lag2']     = grp.shift(2)\ndf['pet_rolling3'] = grp.transform(lambda x: x.rolling(3, min_periods=1).mean())\n\n# Fill PET lags NaN with per-station pet (first obs has no lag)\nfor col in ['pet_lag1', 'pet_lag2', 'pet_rolling3']:\n    df[col] = df[col].fillna(df['pet'])\n\nFEATURE_COLS = [\n    'Latitude', 'Longitude',\n    'nir', 'green', 'swir16', 'swir22',\n    'NDMI', 'MNDWI',\n    'pet', 'pet_lag1', 'pet_lag2', 'pet_rolling3',\n    'month_sin', 'month_cos', 'year',\n    'NIR_SWIR22_ratio', 'turbidity',\n]\nprint(f\"Features ({len(FEATURE_COLS)}): {FEATURE_COLS}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "709338ab",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 5. Winsorize Targets & Prepare Training Data"
    },
    {
      "cell_type": "code",
      "id": "d55f6387",
      "metadata": {
        "language": "python"
      },
      "source": "TARGETS = ['Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n\n# Winsorize extreme outliers before log transform\nfor col in TARGETS:\n    df[col] = mstats.winsorize(df[col], limits=[0.01, 0.01])\n\n# Groups for spatial CV\ngroups = df[['Latitude', 'Longitude']].apply(lambda r: f\"{r['Latitude']}_{r['Longitude']}\", axis=1)\n\n# Sample weight: upweight southern stations (test set is entirely southern)\nsample_weight = np.where(df['Latitude'] < -31.0, 3.0, 1.0)\n\nX = df[FEATURE_COLS]\ny_TA_log  = np.log1p(df['Total Alkalinity'])\ny_EC_log  = np.log1p(df['Electrical Conductance'])\ny_DRP_log = np.log1p(df['Dissolved Reactive Phosphorus'])\n\nprint(f\"Samples: {len(X)} | Unique locations: {groups.nunique()}\")\nprint(f\"Southern samples (weight=3): {(df['Latitude'] < -31.0).sum()} ({(df['Latitude'] < -31.0).mean():.1%})\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d6d2e36d",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 6. Model Training (XGBoost + GroupKFold + sample_weight, no scaler)"
    },
    {
      "cell_type": "code",
      "id": "b96cb571",
      "metadata": {
        "language": "python"
      },
      "source": "def run_pipeline(X, y_log, groups, sample_weight, param_name, n_splits=5, n_iter=50):\n    print(f\"\\n{'='*60}\\nTraining: {param_name}\\n{'='*60}\")\n\n    group_kfold = GroupKFold(n_splits=n_splits)\n\n    # No StandardScaler — XGBoost is scale-invariant\n    model = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1, verbosity=0)\n\n    param_dist = {\n        'n_estimators':     randint(200, 800),\n        'max_depth':        [4, 6, 8],\n        'learning_rate':    uniform(0.05, 0.15),\n        'subsample':        uniform(0.6, 0.4),\n        'colsample_bytree': uniform(0.6, 0.4),\n        'min_child_weight': [1, 2, 3],\n        'reg_alpha':        [0, 0.01, 0.1],\n        'reg_lambda':       [0.5, 1.0, 1.5],\n        'gamma':            [0, 0.01, 0.05],\n    }\n\n    rmse_scorer = make_scorer(\n        lambda yt, yp: np.sqrt(mean_squared_error(np.expm1(yt), np.expm1(yp))),\n        greater_is_better=False)\n    r2_scorer = make_scorer(\n        lambda yt, yp: r2_score(np.expm1(yt), np.expm1(yp)),\n        greater_is_better=True)\n\n    search = RandomizedSearchCV(\n        model, param_dist, n_iter=n_iter, cv=group_kfold,\n        scoring={'r2': r2_scorer, 'rmse': rmse_scorer}, refit='r2',\n        random_state=42, n_jobs=-1, verbose=1, return_train_score=True)\n\n    search.fit(X, y_log, groups=groups, sample_weight=sample_weight)\n\n    best = search.best_estimator_\n    idx = search.best_index_\n    cv = search.cv_results_\n\n    print(f\"\\nBest params: { {k: v for k, v in search.best_params_.items()} }\")\n    print(f\"CV Train R²: {cv['mean_train_r2'][idx]:.4f} | Val R²: {cv['mean_test_r2'][idx]:.4f}\")\n    print(f\"CV Train RMSE: {-cv['mean_train_rmse'][idx]:.4f} | Val RMSE: {-cv['mean_test_rmse'][idx]:.4f}\")\n\n    return best\n\nmodel_TA  = run_pipeline(X, y_TA_log,  groups, sample_weight, \"Total Alkalinity\")\nmodel_EC  = run_pipeline(X, y_EC_log,  groups, sample_weight, \"Electrical Conductance\")\nmodel_DRP = run_pipeline(X, y_DRP_log, groups, sample_weight, \"Dissolved Reactive Phosphorus\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "b40417c4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 7. SHAP Feature Importance"
    },
    {
      "cell_type": "code",
      "id": "7c08f041",
      "metadata": {
        "language": "python"
      },
      "source": "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\nfor ax, model, name in zip(axes,\n                           [model_TA, model_EC, model_DRP],\n                           ['Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']):\n    explainer = shap.TreeExplainer(model)\n    sv = explainer.shap_values(X)\n    plt.sca(ax)\n    shap.summary_plot(sv, X, feature_names=FEATURE_COLS, show=False, plot_size=None)\n    ax.set_title(name, fontsize=12)\nplt.tight_layout()\nplt.show()",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "ff4fd55a",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 8. Prepare Test Set & Predict"
    },
    {
      "cell_type": "code",
      "id": "5db974fd",
      "metadata": {
        "language": "python"
      },
      "source": "landsat_test = pd.read_csv(\"landsat_features_validation.csv\")\nterra_test   = pd.read_csv(\"terraclimate_features_validation.csv\")\nsubmission_template = pd.read_csv(\"submission_template.csv\")\n\ntest = landsat_test.merge(terra_test, on=JOIN_KEYS, how='left')\n\n# Same calibration as training\ntest = calibrate_landsat(test)\n\n# Cloud filter\nfor col in BANDS:\n    test[col] = test[col].where(test[col].between(-0.05, 1.2), np.nan)\n\n# Impute using TRAINING cluster medians (assign test points to nearest train cluster)\ntest['geo_cluster'] = kmeans.predict(test[['Latitude', 'Longitude']])\nfor col in impute_cols:\n    cluster_med = df.groupby('geo_cluster')[col].median()\n    mask = test[col].isna()\n    test.loc[mask, col] = test.loc[mask, 'geo_cluster'].map(cluster_med)\ntest[impute_cols] = test[impute_cols].fillna(train_medians)\n\n# Feature engineering (same as training)\ndates_t = pd.to_datetime(test['Sample Date'], dayfirst=True)\ntest['month_sin'] = np.sin(2 * np.pi * dates_t.dt.month / 12)\ntest['month_cos'] = np.cos(2 * np.pi * dates_t.dt.month / 12)\ntest['year']      = dates_t.dt.year\ntest['NIR_SWIR22_ratio'] = test['nir'] / (test['swir22'] + 1e-6)\ntest['turbidity']        = test['swir16'] / (test['green'] + 1e-6)\n\ntest = test.sort_values(['Latitude', 'Longitude', 'Sample Date']).reset_index(drop=True)\ngrp_t = test.groupby(['Latitude', 'Longitude'])['pet']\ntest['pet_lag1']     = grp_t.shift(1)\ntest['pet_lag2']     = grp_t.shift(2)\ntest['pet_rolling3'] = grp_t.transform(lambda x: x.rolling(3, min_periods=1).mean())\nfor col in ['pet_lag1', 'pet_lag2', 'pet_rolling3']:\n    test[col] = test[col].fillna(test['pet'])\n\nX_test = test[FEATURE_COLS]\nprint(f\"Test shape: {X_test.shape}\")\n\n# Predict (log-space → original scale)\npred_TA  = np.expm1(model_TA.predict(X_test))\npred_EC  = np.expm1(model_EC.predict(X_test))\npred_DRP = np.expm1(model_DRP.predict(X_test))\n\n# Align predictions back to submission_template order\ntest['pred_TA']  = pred_TA\ntest['pred_EC']  = pred_EC\ntest['pred_DRP'] = pred_DRP\n\nsubmission_df = submission_template[JOIN_KEYS].merge(\n    test[JOIN_KEYS + ['pred_TA', 'pred_EC', 'pred_DRP']], on=JOIN_KEYS, how='left'\n).rename(columns={\n    'pred_TA': 'Total Alkalinity',\n    'pred_EC': 'Electrical Conductance',\n    'pred_DRP': 'Dissolved Reactive Phosphorus',\n})\n\ndisplay(submission_df.head())\nprint(f\"\\nNaNs in submission: {submission_df.isna().sum().sum()}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "307db1a4",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## 9. Save & Upload Submission"
    },
    {
      "cell_type": "code",
      "id": "6050a26a",
      "metadata": {
        "language": "python"
      },
      "source": "submission_df.to_csv(\"/tmp/submission.csv\", index=False)\nprint(\"Saved to /tmp/submission.csv\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3e32d89f",
      "metadata": {
        "language": "python"
      },
      "source": "import snowflake\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nsession.sql(\"\"\"\n    PUT file:///tmp/submission.csv\n    'snow://workspace/USER$.PUBLIC.\"ey-hackathon\"/versions/live/'\n    AUTO_COMPRESS=FALSE\n    OVERWRITE=TRUE\n\"\"\").collect()\nprint(\"File saved! Refresh the browser to see the files in the sidebar\")",
      "outputs": [],
      "execution_count": null
    }
  ]
}