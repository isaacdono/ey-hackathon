{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0624afc-f373-48de-a88f-fd70a8cf46b7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell23"
      },
      "source": [
        "## 2026 EY AI & Data Challenge - Landsat Data Extraction Notebook\n",
        "\n",
        "This notebook demonstrates Landsat data extraction and the creation of an output file to be used by the benchmark notebook. The baseline data is [Landsat Collection 2 Level 2](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) data from the MS Planetary Computer catalog.\n",
        "\n",
        "**Caution**... This notebook requires significant execution time as there are 9,319 data points (unique locations and times) used for data extraction from the Landsat archive. The code takes about 7 hours to run to completion on a typical laptop computer with a typical internet connection. Lower execution times are likely possible with optimization of the data extraction process and the use of cloud computing services.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf044936-9aad-4300-a873-ddf8d2b43835",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell2"
      },
      "source": [
        "### Load In Dependencies\n",
        "The following code installs the required Python libraries (found in the requirements.txt file) in the Snowflake environment to allow successful execution of the remaining notebook code. After running this code for the first time, it is required to ‚Äúrestart‚Äù the kernal so the Python libraries are available in the environment. This is done by selecting the ‚ÄúConnected‚Äù menu above the notebook (next to ‚ÄúRun all‚Äù) and selecting the ‚Äúrestart kernal‚Äù link. Subsequent runs of the notebook do not require this ‚Äúrestart‚Äù process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0759bc3-bff9-4cfe-83d0-c1f808697c9d",
      "metadata": {
        "language": "python",
        "name": "cell26"
      },
      "outputs": [],
      "source": [
        "!pip install uv\n",
        "!uv pip install  -r requirements.txt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a09f2097-d79a-4a87-9977-79a85b8e651b",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell3"
      },
      "outputs": [],
      "source": [
        "import snowflake\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Planetary Computer tools for STAC API access and authentication\n",
        "import pystac_client\n",
        "import planetary_computer as pc\n",
        "from odc.stac import stac_load\n",
        "from pystac.extensions.eo import EOExtension as eo\n",
        "\n",
        "from datetime import date\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f6736c-ac2e-44d2-bded-d3266897074e",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell1"
      },
      "source": [
        "### Extracting Landsat Data Using API Calls\n",
        "\n",
        "The API-based method allows us to efficiently access **Landsat** data for specific coordinates and time periods, ensuring scalability and reproducibility of the process.\n",
        "\n",
        "Through the API, we can query individual bands or compute indices like **NDMI** on the fly. This approach reduces storage requirements and simplifies data preprocessing, making it ideal for large-scale environmental and water quality analysis.\n",
        "\n",
        "The **compute_Landsat_values** function extracts Landsat surface reflectance values for specific sampling locations using a 100 m focal buffer around each point. For each location:\n",
        "\n",
        "- A bounding box (bbox) is created around the latitude and longitude coordinates.\n",
        "- The Microsoft Planetary Computer API is queried for Landsat-8 Level-2 surface reflectance imagery within the date range.\n",
        "- The nearest low-cloud (<10% cloud cover) scene is selected, and the specified bands (**green**, **nir08**, **swir16**, **swir22**) are loaded.\n",
        "- Median values of the pixels within the bounding box are computed to reduce the effect of noise or outliers.\n",
        "\n",
        "**Why the buffer value is 0.00089831**\n",
        "\n",
        "We want a ~100 m buffer around each point.  \n",
        "At the equator, 1 degree ‚âà 110 km.\n",
        "\n",
        "Therefore, the degree equivalent of 100 m is:\n",
        "\n",
        "*buffer_deg ‚âà 100 m / 110,000 m per degree ‚âà 0.00089831*\n",
        "\n",
        "This value ensures that the buffer approximately matches the pixel resolution of Landsat imagery, capturing a ~100 m area around each sampling location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dcd447c8-2951-4391-a5c8-916ce9666306",
      "metadata": {
        "language": "python",
        "name": "cell5"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "tqdm.pandas()\n",
        "\n",
        "def compute_Landsat_values(row, sleep_sec=0.5):\n",
        "    lat = row['Latitude']\n",
        "    lon = row['Longitude']\n",
        "    date = pd.to_datetime(row['Sample Date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "    bbox_size = 0.00089831\n",
        "    bbox = [\n",
        "        lon - bbox_size / 2,\n",
        "        lat - bbox_size / 2,\n",
        "        lon + bbox_size / 2,\n",
        "        lat + bbox_size / 2\n",
        "    ]\n",
        "\n",
        "    # Rate-limiting: pause before each API call\n",
        "    time.sleep(sleep_sec)\n",
        "\n",
        "    catalog = pystac_client.Client.open(\n",
        "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
        "        modifier=pc.sign_inplace,\n",
        "    )\n",
        "\n",
        "    search = catalog.search(\n",
        "        collections=[\"landsat-c2-l2\"],\n",
        "        bbox=bbox,\n",
        "        datetime=\"2011-01-01/2015-12-31\",\n",
        "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
        "    )\n",
        "\n",
        "    items = search.item_collection()\n",
        "\n",
        "    NAN_RESULT = pd.Series({\n",
        "        \"blue\": np.nan, \"green\": np.nan, \"red\": np.nan,\n",
        "        \"nir\": np.nan, \"swir16\": np.nan, \"swir22\": np.nan\n",
        "    })\n",
        "\n",
        "    if not items:\n",
        "        return NAN_RESULT\n",
        "\n",
        "    try:\n",
        "        sample_date_utc = date.tz_localize(\"UTC\") if date.tzinfo is None else date.tz_convert(\"UTC\")\n",
        "\n",
        "        items = sorted(\n",
        "            items,\n",
        "            key=lambda x: abs(pd.to_datetime(x.properties[\"datetime\"]).tz_convert(\"UTC\") - sample_date_utc)\n",
        "        )\n",
        "        selected_item = pc.sign(items[0])\n",
        "\n",
        "        bands_of_interest = [\"blue\", \"green\", \"red\", \"nir08\", \"swir16\", \"swir22\"]\n",
        "        data = stac_load([selected_item], bands=bands_of_interest, bbox=bbox).isel(time=0)\n",
        "\n",
        "        medians = {}\n",
        "        band_map = {\"blue\": \"blue\", \"green\": \"green\", \"red\": \"red\",\n",
        "                    \"nir\": \"nir08\", \"swir16\": \"swir16\", \"swir22\": \"swir22\"}\n",
        "\n",
        "        for out_name, band_key in band_map.items():\n",
        "            val = float(data[band_key].astype(\"float\").median(skipna=True).values)\n",
        "            medians[out_name] = val if val != 0 else np.nan\n",
        "\n",
        "        return pd.Series(medians)\n",
        "\n",
        "    except Exception:\n",
        "        return NAN_RESULT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a4ea22-9d2a-4866-bc59-bf3962ecfe1a",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell6"
      },
      "source": [
        "### Extracting features for the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c046f70-aa61-4bbe-8b1a-ac8624f87ed9",
      "metadata": {
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "Water_Quality_df=pd.read_csv('water_quality_training_dataset.csv')\n",
        "display(Water_Quality_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f33406f9-9c07-400b-95a5-cdfdc81c5eba",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell8"
      },
      "outputs": [],
      "source": [
        "Water_Quality_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8b49cf-b76b-4633-be3e-caf4354ff0ef",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell4"
      },
      "source": [
        "### Note\n",
        "\n",
        "The Landsat data extraction process for all 9,319 locations typically requires more than 7 hours when executed in a single run. During long executions, you may occasionally encounter API limits, timeout errors, or request failures. To avoid these interruptions, we recommend running the extraction in smaller batches.\n",
        "\n",
        "In this notebook, we provide a sample code snippet demonstrating how to extract data for the first 200 locations. Participants are encouraged to follow the same batching approach to extract data for all 9,319 locations safely and efficiently.\n",
        "\n",
        "We have already executed the full extraction for all 9,319 locations and saved the output to **landsat_features_training.csv**, which will be used in the benchmark notebook.  \n",
        "Similarly, participants can extract Landsat features in batches, combine the batch outputs, and save the final merged dataset as **landsat_features_training.csv** to ensure the benchmark notebook runs smoothly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2796f7f7-1cba-4c2d-880b-625c9dd3504d",
      "metadata": {
        "language": "python",
        "name": "cell27"
      },
      "outputs": [],
      "source": [
        "Water_Quality_df = pd.read_csv('water_quality_training_dataset.csv')\n",
        "\n",
        "chunksize       = 500          # rows per batch\n",
        "sleep_between   = 2            # seconds to wait between batches\n",
        "output_path     = \"landsat_features_training_full.csv\"\n",
        "\n",
        "dfs = []\n",
        "total_batches = (len(Water_Quality_df) + chunksize - 1) // chunksize\n",
        "\n",
        "print(f\"üöÄ Starting Landsat feature extraction for training data...\")\n",
        "print(f\"   Total rows: {len(Water_Quality_df)} | Batch size: {chunksize} | Batches: {total_batches}\\n\")\n",
        "\n",
        "for batch_num, i in enumerate(tqdm(range(0, len(Water_Quality_df), chunksize),\n",
        "                                   desc=\"Batches\", total=total_batches), start=1):\n",
        "    chunk = Water_Quality_df.iloc[i : i + chunksize]\n",
        "\n",
        "    print(f\"\\n‚è≥ Batch {batch_num}/{total_batches} ‚Äî rows {i} to {i + len(chunk) - 1}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    chunk_features = chunk.progress_apply(compute_Landsat_values, axis=1)\n",
        "    dfs.append(chunk_features)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"   ‚úÖ Batch {batch_num} done in {elapsed:.1f}s\")\n",
        "\n",
        "    # Checkpoint: save incrementally so no work is lost on failure\n",
        "    partial = pd.concat(dfs, ignore_index=True)\n",
        "    partial.to_csv(output_path, index=False)\n",
        "    print(f\"   üíæ Checkpoint saved ‚Üí {output_path} ({len(partial)} rows so far)\")\n",
        "\n",
        "    if batch_num < total_batches:\n",
        "        print(f\"   üí§ Sleeping {sleep_between}s before next batch...\")\n",
        "        time.sleep(sleep_between)\n",
        "\n",
        "landsat_train_features = pd.concat(dfs, ignore_index=True)\n",
        "print(f\"\\nüéâ Extraction complete! Total rows: {len(landsat_train_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400e9ddb-81f6-45eb-ad41-e1950f6f4eac",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell12"
      },
      "source": [
        "**NDMI and MNDWI Indices**\n",
        "\n",
        "In this notebook, we compute two commonly used water-related indices from the extracted Landsat bands:\n",
        "\n",
        "- **NDMI (Normalized Difference Moisture Index):**  \n",
        "  Measures vegetation water content and surface moisture.  \n",
        "  Computed as *(NIR - SWIR16) / (NIR + SWIR16)*.\n",
        "\n",
        "- **MNDWI (Modified Normalized Difference Water Index):**  \n",
        "  Highlights open water features by enhancing water reflectance and suppressing built-up areas.  \n",
        "  Computed as *(Green - SWIR16) / (Green + SWIR16)*.\n",
        "\n",
        "An **epsilon value** (*eps = 1e-10*) is added to the denominators to avoid division by zero.  \n",
        "These indices are widely used in hydrological and water quality analyses for detecting water presence and vegetation moisture levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "31ca3eaa-45f9-44af-b444-2dff934d02f4",
      "metadata": {
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "# Create indices: NDMI and MNDWI\n",
        "eps = 1e-10\n",
        "landsat_train_features['NDMI'] = (landsat_train_features['nir'] - landsat_train_features['swir16']) / (landsat_train_features['nir'] + landsat_train_features['swir16'] + eps)\n",
        "landsat_train_features['MNDWI'] = (landsat_train_features['green'] - landsat_train_features['swir16']) / (landsat_train_features['green'] + landsat_train_features['swir16'] + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d9d6711-3553-437a-8830-db69ce8afc80",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell14"
      },
      "outputs": [],
      "source": [
        "landsat_train_features['Latitude'] = Water_Quality_df['Latitude']\n",
        "landsat_train_features['Longitude'] = Water_Quality_df['Longitude']\n",
        "landsat_train_features['Sample Date'] = Water_Quality_df['Sample Date']\n",
        "landsat_train_features = landsat_train_features[['Latitude', 'Longitude', 'Sample Date', 'nir','nir08', 'red', 'blue', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f72aca-7bfe-4016-8757-b0541ed8060b",
      "metadata": {
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": [
        "# Preview File\n",
        "landsat_train_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dc6075a-180c-4915-976f-a17846b32473",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "landsat_train_features.to_csv(\"/tmp/landsat_features_training.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e89a601-832c-451a-baf2-ed265817c1f8",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "session.sql(\"\"\"\n",
        "    PUT file:///tmp/landsat_features_training.csv\n",
        "    'snow://workspace/USER$.PUBLIC.\"ey-hackathon\"/versions/live/'\n",
        "    AUTO_COMPRESS=FALSE\n",
        "    OVERWRITE=TRUE\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"File saved! Refresh the browser to see the files in the sidebar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f44c9f-2336-4fe7-a587-78cf6efe694b",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac5f5ea-bd8a-41bc-bb62-47cc952952bf",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "cell17"
      },
      "source": [
        "### Extracting features for the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb2adc08-4661-4fdc-a72d-78d34d37db73",
      "metadata": {
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "Validation_df=pd.read_csv('submission_template.csv')\n",
        "display(Validation_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "53ebf740-1917-4af1-b214-d60372f0ad0b",
      "metadata": {
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "Validation_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f104ef32-7021-4d05-8ff0-1b0bb18e93c0",
      "metadata": {
        "language": "python",
        "name": "cell28"
      },
      "outputs": [],
      "source": [
        "chunksize       = 500          # rows per batch\n",
        "sleep_between   = 2            # seconds to wait between batches\n",
        "val_output_path = \"landsat_features_validation_full.csv\"\n",
        "\n",
        "val_dfs = []\n",
        "total_batches = (len(Validation_df) + chunksize - 1) // chunksize\n",
        "\n",
        "print(f\"üöÄ Starting Landsat feature extraction for validation data...\")\n",
        "print(f\"   Total rows: {len(Validation_df)} | Batch size: {chunksize} | Batches: {total_batches}\\n\")\n",
        "\n",
        "for batch_num, i in enumerate(tqdm(range(0, len(Validation_df), chunksize),\n",
        "                                   desc=\"Batches\", total=total_batches), start=1):\n",
        "    chunk = Validation_df.iloc[i : i + chunksize]\n",
        "\n",
        "    print(f\"\\n‚è≥ Batch {batch_num}/{total_batches} ‚Äî rows {i} to {i + len(chunk) - 1}\")\n",
        "    t0 = time.time()\n",
        "\n",
        "    chunk_features = chunk.progress_apply(compute_Landsat_values, axis=1)\n",
        "    val_dfs.append(chunk_features)\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    print(f\"   ‚úÖ Batch {batch_num} done in {elapsed:.1f}s\")\n",
        "\n",
        "    # Checkpoint: save incrementally so no work is lost on failure\n",
        "    partial = pd.concat(val_dfs, ignore_index=True)\n",
        "    partial.to_csv(val_output_path, index=False)\n",
        "    print(f\"   üíæ Checkpoint saved ‚Üí {val_output_path} ({len(partial)} rows so far)\")\n",
        "\n",
        "    if batch_num < total_batches:\n",
        "        print(f\"   üí§ Sleeping {sleep_between}s before next batch...\")\n",
        "        time.sleep(sleep_between)\n",
        "\n",
        "landsat_val_features = pd.concat(val_dfs, ignore_index=True)\n",
        "print(f\"\\nüéâ Extraction complete! Total rows: {len(landsat_val_features)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b130a364-1778-457e-ace5-b121dfcc7c7c",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "# Create indices: NDMI and MNDWI\n",
        "eps = 1e-10\n",
        "landsat_val_features['NDMI'] = (landsat_val_features['nir'] - landsat_val_features['swir16']) / (landsat_val_features['nir'] + landsat_val_features['swir16'])\n",
        "landsat_val_features['MNDWI'] = (landsat_val_features['green'] - landsat_val_features['swir16']) / (landsat_val_features['green'] + landsat_val_features['swir16'] + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc7fe05a-2bfd-4d0f-a796-48a1e2f3b3c0",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell22"
      },
      "outputs": [],
      "source": [
        "landsat_val_features['Latitude'] = Validation_df['Latitude']\n",
        "landsat_val_features['Longitude'] = Validation_df['Longitude']\n",
        "landsat_val_features['Sample Date'] = Validation_df['Sample Date']\n",
        "landsat_val_features = landsat_val_features[['Latitude', 'Longitude', 'Sample Date', 'nir','nir08', 'red', 'blue', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7aa733-736c-4db7-840c-9e47e332348b",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell16"
      },
      "outputs": [],
      "source": [
        "# Preview File\n",
        "landsat_val_features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e81285-b2a7-4b6e-907c-4fa273db3bba",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "landsat_val_features.to_csv(\"/tmp/landsat_features_validation.csv\",index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d753f1e1-0346-4c46-a0fa-1441850aa35f",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "session.sql(\"\"\"\n",
        "    PUT file:///tmp/landsat_features_validation.csv\n",
        "    'snow://workspace/USER$.PUBLIC.\"ey-hackathon\"/versions/live/'\n",
        "    AUTO_COMPRESS=FALSE\n",
        "    OVERWRITE=TRUE\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"File saved! Refresh the browser to see the files in the sidebar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c287302-e015-4dff-b328-e8f5a639f9f8",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false
      },
      "source": [
        "**Note:** If you're using your own workspace, remember to replace \"EY-AI-and-Data-Challenge\" with your workspace name in the file path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f396bb98-69f9-4893-b545-f3c873cb45ee",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
